{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04cbf2a0-5c09-486b-950f-50391ec2ad44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define os imports necessários para a execução do código\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as fn\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.core.display import HTML\n",
    "import os, time, json\n",
    "from requests import post\n",
    "\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))\n",
    "\n",
    "# Define a sessão do Spark com os jars necessários para conexão com o MINIO\n",
    "spark = (SparkSession.builder\n",
    "         .config(\"spark.jars\",\"\"\"/home/jovyan/jars/aws-java-sdk-core-1.11.534.jar,\n",
    "                                 /home/jovyan/jars/aws-java-sdk-dynamodb-1.11.534.jar,\n",
    "                                 /home/jovyan/jars/aws-java-sdk-s3-1.11.534.jar,\n",
    "                                 /home/jovyan/jars/hadoop-aws-3.2.2.jar\"\"\")\n",
    "         .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "         .config(\"spark.hadoop.fs.s3a.access.key\", \"aulafia\")\n",
    "         .config(\"spark.hadoop.fs.s3a.secret.key\", \"aulafia@123\")\n",
    "         .config(\"spark.hadoop.fs.s3a.path.style.access\", True)\n",
    "         .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "         .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "         .getOrCreate()\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc18ce55-6834-492e-b303-84223de7653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_ingestion = 1688277723.87854"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13604cfa-ea7e-4f49-9a58-b6f00c7da2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-07-06\n"
     ]
    }
   ],
   "source": [
    "# Lista de Endpoints a serem carregados\n",
    "endpoints = [\"games\"]\n",
    "\n",
    "# Define a data e hora do início da extração para a tabela de controle de atualizações \n",
    "extraction_time = time.time()\n",
    "\n",
    "# Define a data de extração para particionamento no Lake\n",
    "extraction_date = time.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Dados de acesso e URL da API\n",
    "client_id = \"rtpo9gi65ed6me23fslamjdmqme962\"\n",
    "authorization = \"ojr9ucic9n7670dz3sbdba5kb2wa06\"\n",
    "api_prefix = \"IGDB_\"\n",
    "url = 'https://api.igdb.com/v4/'\n",
    "\n",
    "# Limite de registros a serem lidos por request da API\n",
    "data_retrieve_limit = 500\n",
    "\n",
    "# Query para solicitação dos dados\n",
    "query = 'fields *; where updated_at > ' + str(last_ingestion) + '; limit ' + str(data_retrieve_limit) + '; sort id asc;'\n",
    "\n",
    "# Limite de requests a serem feitos por segundo (por limitação da API)\n",
    "rate_limit = 0 # 1 / 4 (desabilitado em função do delay do S3)\n",
    "\n",
    "print(extraction_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d651522a-77a6-49b8-bf6a-97111910982c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando endpoint: games\n",
      "Foram importados 476 arquivos json para o endpoint 'games'\n",
      "Carga Incremental finalizada! Processo executado em 00:56:51.\n"
     ]
    }
   ],
   "source": [
    "# Executa um loop para varrer todos os endpoints e obter seus dados\n",
    "\n",
    "inicio_exec = time.time()\n",
    "\n",
    "for endpoint in endpoints:\n",
    "\n",
    "    print(\"Iniciando endpoint: \" + endpoint) \n",
    "    \n",
    "    page = 0\n",
    "    offset = 0\n",
    "    \n",
    "    url_endpoint = url + endpoint\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        # Atualiza a query com o offset\n",
    "        query_page = query + ' offset ' + str(offset) + ';'\n",
    "\n",
    "        # Define os parâmetros da requisição        \n",
    "        request = {'headers': {'Client-ID': client_id, 'Authorization': 'Bearer ' + authorization},'data': query_page}\n",
    "\n",
    "        # Realiza a chamada via método POST\n",
    "        response = post(url_endpoint, **request)\n",
    "\n",
    "        # Verificar o código de status da resposta\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "        else:\n",
    "            print(\"Erro na requisição: \" + response.status_code)\n",
    "            break\n",
    "\n",
    "        # Verifica se a resposta está vazia (fim dos dados)      \n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        # Incrementa o contador de páginas\n",
    "        page += 1\n",
    "        \n",
    "        # Cria o dataframe com os dados extraídos\n",
    "        df = spark.createDataFrame(data)\n",
    "        \n",
    "        # Salva o dataframe em um arquivo json no diretório especificado\n",
    "        df.write.json('s3a://landing-zone/igdb/' + endpoint + '/' + extraction_date + '/' + api_prefix + endpoint + '_page_' + str(page).zfill(3) + '.json', mode='overwrite')\n",
    "\n",
    "        # Atualiza o offset para a próxima requisição e aguarda o rate limit      \n",
    "        offset += data_retrieve_limit\n",
    "        time.sleep(rate_limit)\n",
    "\n",
    "    print(f\"Foram importados {page} arquivos json para o endpoint '{endpoint}'\")\n",
    "\n",
    "fim_exec = time.time()\n",
    "tempo_exec = fim_exec - inicio_exec\n",
    "\n",
    "# Calcula o tempo de execução\n",
    "horas = int(tempo_exec // 3600)\n",
    "minutos = int((tempo_exec % 3600) // 60)\n",
    "segundos = int(tempo_exec % 60)\n",
    "\n",
    "print(f\"Carga Incremental finalizada! Processo executado em {horas:02d}:{minutos:02d}:{segundos:02d}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
